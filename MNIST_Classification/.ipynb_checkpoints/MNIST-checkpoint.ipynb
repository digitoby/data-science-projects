{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ccf7893-ff2e-4914-b018-537017f61282",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0aa821d-319b-463c-86f1-4fffe48beec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30c649-b5d0-4e1a-bdb3-7a017ea10963",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71c83022-b567-4aa9-b45b-4903c520c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the data\n",
    "#with_info=True provides useful metadata\n",
    "#as_supervised=True will load the dataset in a 2-tuple structure (input, target)\n",
    "mnist_data, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "#create our data variables\n",
    "mnist_train, mnist_test = mnist_data['train'], mnist_data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "263a27f6-95b8-4fc6-a9b4-e1f755ba5d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_dir='C:\\\\Users\\\\tobst\\\\tensorflow_datasets\\\\mnist\\\\3.0.1',\n",
       "    file_format=tfrecord,\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    nondeterministic_order=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3e7d378-aea4-41c8-b362-bae8ab482de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create variables for data allocation (train, validate, test)\n",
    "#cast them to int just in case\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ae92e66-df06-4c3f-a719-74a7cb8f80e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the data between 0 and 1\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.int64)\n",
    "    image /= 255 #255 since values range from 0-255\n",
    "    return image, label\n",
    "\n",
    "scaled_train_validation = mnist_train.map(scale)\n",
    "scaled_test = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ce005a6-1d56-419c-aadc-7e9c77092789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the training data\n",
    "buffer_size = 10000\n",
    "\n",
    "shuffled_train_validation = scaled_train_validation.shuffle(buffer_size)\n",
    "\n",
    "#create the train and validation datasets\n",
    "validation = shuffled_train_validation.take(num_validation_samples)\n",
    "train = shuffled_train_validation.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d7ea9e85-57e2-4e36-91a1-36ac9a83e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create batches for model training and compatability\n",
    "batch_size = 100\n",
    "\n",
    "train = train.batch(batch_size)\n",
    "validation = validation.batch(num_validation_samples)\n",
    "test = scaled_test.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebc5aefc-869a-4929-ac8b-b45629f40417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split validation data for validation testing later\n",
    "#use next(iter()) because validation is a tf.data.Dataset/iterable list of batches since we batched it earlier\n",
    "validation_inputs, validation_targets = next(iter(validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7fef2-c09e-4e75-b3a3-e72c769c7b52",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8f1d0bb-170a-4587-912d-4f3a56715d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the neural network\n",
    "input_size = 784 #784 because 28^2\n",
    "output_size = 10 #10 beause 10 possible targets\n",
    "hidden_layer_width = 350\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    #first declare the input layer\n",
    "    tf.keras.Input(shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Flatten(), #flatten since our hidden layers can not take 2D objects but 1D, so vectors\n",
    "    tf.keras.layers.Dense(hidden_layer_width, activation='relu'), #1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_width, activation='relu'), #2nd hidden layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') #output layer, use softmax since we want proportions for classification\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f731276-4137-4939-85ae-87110df5e233",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "497faeda-e75c-4f95-9db2-aaf6b221ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "#adam since it is optimal\n",
    "#sparse_categorical_crossentropy since our data is not already one-hot encoded\n",
    "model.compile(custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b63b0-f054-4206-8375-30281eea5000",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7df9f90f-9392-462d-96fb-70fe06cca110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "540/540 - 3s - 6ms/step - accuracy: 0.9309 - loss: 0.2375 - val_accuracy: 0.9638 - val_loss: 0.1122\n",
      "Epoch 2/7\n",
      "540/540 - 2s - 4ms/step - accuracy: 0.9731 - loss: 0.0890 - val_accuracy: 0.9783 - val_loss: 0.0769\n",
      "Epoch 3/7\n",
      "540/540 - 2s - 4ms/step - accuracy: 0.9817 - loss: 0.0571 - val_accuracy: 0.9817 - val_loss: 0.0542\n",
      "Epoch 4/7\n",
      "540/540 - 2s - 4ms/step - accuracy: 0.9859 - loss: 0.0429 - val_accuracy: 0.9882 - val_loss: 0.0398\n",
      "Epoch 5/7\n",
      "540/540 - 2s - 4ms/step - accuracy: 0.9897 - loss: 0.0309 - val_accuracy: 0.9855 - val_loss: 0.0418\n",
      "Epoch 6/7\n",
      "540/540 - 2s - 4ms/step - accuracy: 0.9916 - loss: 0.0268 - val_accuracy: 0.9923 - val_loss: 0.0290\n",
      "Epoch 7/7\n",
      "540/540 - 2s - 4ms/step - accuracy: 0.9930 - loss: 0.0207 - val_accuracy: 0.9907 - val_loss: 0.0298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f67b33ebf0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 7\n",
    "\n",
    "model.fit(train, epochs=num_epochs, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0178cbcb-3af3-45e9-8178-f53716f27ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.9802 - loss: 0.0724\n",
      "Test loss: 0.07. Test accuracy: 98.02%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test)\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.20]",
   "language": "python",
   "name": "conda-env-py3-TF2.20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
